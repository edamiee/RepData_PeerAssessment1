ex2_fair- ex2_fair*ex2_fair
ex2_fair-3.5^2
sum(edh)-3.5^2
sum(dice_high * dice_sqr)-edh^2
sd(apply(matrix(rnorm(10000),1000),1,mean))
1/sqrt(10)
1/sqrt(120)
sd(apply(matrix(runif(10000),1000),1,mean))
2/sqrt(10)
sd(apply(matrix(rpois(10000,4),1000),1,mean))
1/(2*sqrt(10))
sd(apply(matrix(sample(0:1,10000,TRUE),1000),1,mean))
.24
choose(5,3)*(.8)^x*(.2)(5-3)
choose(5,3)*(.8)^3*(.2)(5-3)
?choose
choose(5,3)*(.8)^3*(.2)(2)
choose(5,3)
10*8^3*.2*3
3072 +choose(5,4)*.8^4*.2
skip()
?pbinom
pbinom(2,5,.8,lower.tail=FALSE)
?qnorm
qnorm(10,mean=0,sd-1 lower.tail=TRUE,log.p=FALSE)
qnorm(10,mean=0,sd=1 lower.tail=TRUE,log.p=FALSE)
qnorm(.10,mean=0,sd=1, lower.tail=TRUE,log.p=FALSE)
0
qnorm(.975,mean=3,sd=2, lower.tail=TRUE,log.p=FALSE)
3+.975*1.96
1.96*2+3
?pnorm
pnorm(1200,mean=1020,sd=50,lower.tail=FALSE,log.p=FALSE)
pnorm(1200,mean=0,sd=1,lower.tail=FALSE,log.p=FALSE)
pnorm(180/50,lower.tail=FALSE,log.p=FALSE)
pnorm((1200-1020)/50,lower.tail=FALSE)
qnorm(.75,mean=1020,sd=50, lower.tail=TRUE,log.p=FALSE)
.75
pnorm(qnorm(.53))
qnorm(pnorm(.53))
?ppois
ppois(3,2.5*4,lower.tail=TRUE,log.p=FALSE)
?pbinom
pbinom(5, 1000, .01, lower.tail = TRUE, log.p = FALSE)
ppois(5,n*p,lower.tail=TRUE,log.p=FALSE)
?lambda
?ppois
ppois(5,lambda=1000,lower.tail=TRUE,log.p=FALSE)
ppois(5,1000*.01)
?coinPlot
?coinPlot()
coinPlot(10)
coinPLot(10000)
coinPlot(1000)
coinPlot(10000)
?qnorm
qnorm(.95, lower.tail = TRUE, log.p = FALSE)
skip()
info()
bye()
?qnorm
?rnorm
qnorm(.95,mean=100,sd=10,lower.tail = TRUE, log.p = FALSE )
qnorm(.05,mean=100,sd=10,lower.tail = TRUE, log.p = FALSE )
qnorm(.05,mean=100,sd=.2,lower.tail = TRUE, log.p = FALSE )
?choose
choose(5,6)
choose(6,5)
(1+2+3+4+5+6)/6
?poission
?poisson
?ppois
?pnorm
pnorm( 70, mean=80, sd =10, lower.tail = TRUE, log.p = FALSE)
qnorm(.95,mean=1100,sd=75)
qnorm(.95,mean=100,sd=75)
?pbinom
pbinom(4, size=5, .5, lower.tail = TRUE, log.p = FALSE)
pbinom(4, size=5, .5, lower.tail = FALSE, log.p = FALSE)
pbinom(4, size=5, prob=0.5, lower.tail = FALSE)
pbinom(3, size=5, prob=0.5, lower.tail = FALSE)
?ppois
ppois(10,lambda=15)
install.packages('knitr', dependencies = TRUE)
library(knitr)
?knit
knit(input)
?knitr
library(knitr)
?knitr
?md
devtools::install_github("rstudio/rmarkdown")
library(devtools)
devtools::install_github("rstudio/rmarkdown")
install.packages("devtools")
devtools::install_github("rstudio/rmarkdown")
sudo tlmgr update --self
sudo tlmgr install framed
?t.test
t.test(1100,30)
qnorm(0.95, mean = 1100, sd = 30)
ppois?
?ppois
ppois(.95, -2)
ppois(.95, 2)
n1 <- n2 <- 9
x1 <- -3  ##treated
x2 <- 1  ##placebo
s1 <- 1.5  ##treated
s2 <- 1.8  ##placebo
spsq <- ( (n1 - 1) * s1^2 + (n2 - 1) * s2^2) / (n1 + n2 - 2)
spsq
1100 + qt(p=.975,df=9-1)*c(-1,1)*(30/sqrt(9))
n_new = n_old = 100
med_avg_new= 4
med_avg_old = 6
med_var_new = .5^2
med_var_old = 2^2
med_avg_between = med_avg_new - med_avg_old
df = ((med_var_new / n_new + med_var_old / n_old)^2) /
((med_var_new/n_new)^2/(n_new-1) + (med_var_old/n_old)^2/(n_old-1))
t = qt(p=.975,df=df)
-med_avg_between + c(-1,1)*sqrt(med_var_new/n_new + med_var_old/n_old)*t
n_new = n_old = 9
med_avg_new= -3
med_avg_old = 1
med_var_new = 1.5^2
med_var_old = 1.8^2
t_confident_interval = .9
tp = (1-t_confident_interval)/2 + t_confident_interval
med_avg_between = med_avg_new - med_avg_old
df = ((med_var_new / n_new + med_var_old / n_old)^2) /
((med_var_new/n_new)^2/(n_new-1) + (med_var_old/n_old)^2/(n_old-1))
t = qt(p=tp,df=df)
med_avg_between + c(-1,1)*sqrt(med_var_new/n_new + med_var_old/n_old)*t
library(ToothGrowth)
data(ToothGrowth)
t2 <- t.test(len~supp, paired=F, var.equal=F, data=ToothGrowth)
t2
t1 <- t.test(len~supp, paired=F, var.equal=T, data=ToothGrowth)
t1
df
library(swirl)
swirl(0)
myplot(34)
myplot(33.3)
myplot(30)
myplot(28)
z<- qnorm(.95)
?pnorm
pnorm(30+z,mean=30,sd=1, lower.tail=FALSE)
pnorm(30+z,mean=0,sd=1, lower.tail=FALSE)
pnorm(30+z,mean=32, lower.tail=FALSE)
pnorm(30+z,mean=32, lower.tail=FALSE)
pnorm(30+z*2,mean=32,sd=2, lower.tail=FALSE)
power.t.test(n = 16, delta = 2 / 4, sd=1, type ="one.sample", alt = "one.sided")$power
power.t.test(n = 16, delta = 2 , sd=4, type ="one.sample", alt = "one.sided")$power
power.t.test(n = 16, delta = 100 , sd=200, type ="one.sample", alt = "one.sided")$power
power.t.test(n = .8, delta = 2/4 , sd=1, type ="one.sample", alt = "one.sided")$power
power.t.test(n = .8, delta = 2/4 , sd=1, type ="one.sample", alt = "one.sided")$n
power.t.test(power= .8, delta = 2/4 , sd=1, type ="one.sample", alt = "one.sided")$n
power.t.test(power= .8, delta = 2 , sd=4, type ="one.sample", alt = "one.sided")$n
power.t.test(power= .8, delta = 100 , sd=200, type ="one.sample", alt = "one.sided")$n
power.t.test(power= .8, n=26 , sd=1, type ="one.sample", alt = "one.sided")$delta
power.t.test(power= .8, n=27 , sd=1, type ="one.sample", alt = "one.sided")$delta
pValues
head(pValues)
sum(pValues<.05)
p.adjust(pValues,method="bonferroni")
sum(p.adjust(pValues,method="bonferroni") < 0.05)
sum(p.adjust(pValues,method="BH") < 0.05)
tail(trueStatus)
table(pValues2<0.5, trueStatus)
table(pValues2 < 0.05, trueStatus)
p.adjust(pValues2,method="bonferroni")
24/500
table(p.adjust(pValue2,method="bonferroni"))
table(p.adjust(pValues2,method="bonferroni"))
table(p.adjust(pValues2,method="bonferroni") < 0.05, trueStatus)
table(p.adjust(pValues2,method="BH") < 0.05, trueStatus)
sum(50*1/6)
um(1\:6)/6
sum(1\:6)/6
sum(1:6)/6
print(g2)
head(sh)
nh
median(resampledMedians)
median(sh)
sam<- sample(fh,nh*B)
sam<- sample(fhnh*B, replace=TRUE)
sam<- sample(nh*B, replace=TRUE)
sam <- sample(fh,nh*B,replace=TRUE)
resam<-matrix(sam,rows=B,columns=nh)
resam<-matrix(sam)
resam <- matrix(sam,B,nh)
meds<-apply(resam,1,median)
difference(fh,meds)
diff(fh,meds)
fh-meds
median(meds)-median(fh)
sd(meds)-sd(fh)
sd(meds)
sd(resampledMedians)
t.test
quantile(resampledMedians,c(.025,.975))
quantile(meds,c(.025,.975))
dim(InsectSprays)
names(InsectSprays)
range(Bdata$count)
range(Cdata)
range(Cdata$count)
Bounts
BCounts
BCcounts
group
testStat
obs<-testStat(BCcounts,group)
obs
mean(Bdata$count-Cdata$count)
sample(group)
perms <- sapply(1 : 10000, function(i) testStat(BCcounts, sample(group)))
mean(expression perms)
mean(expression perms>obs)
mean(perms>obs)
obs
fsdfds
testStat(DEcounts,group)
perms <- sapply(1 : 10000, function(i) testStat(DEcounts, sample(group)))
datasets(mtcars)
dataset(mtcars)
library(datasets)
data(mtcars)
head(mtcars)
?t.test
t.test(mtcars$mpg,data=mtcars,paired=TRUE)
t.test(mtcars$mpg=4,mtcars$mpg=6,data=mtcars,paired=TRUE)
t.test(mtcars$mpg=4, var.equal=FALES,paired=FALSE)
m6 <- mtcars$mpg[mtcars$cyl == 6]
m8 <- mtcars$mpg[mtcars$cyl == 8]
p <- t.test(m6, m8, paired = FALSE, alternative="two.sided", var.equal=FALSE)$p.value
p
pbinom?
c
?pbinom
a <- c(140, 138, 150, 148, 135)
b <- c(132, 135, 151, 146, 130)
t.test(a, b, alternative = "two.sided", paired = T)
?test?
t.test(a, b, alternative = "two.sided", paired = T)
?t.test
?qnorm
qnorm(.05, mean = 1100, sd = 30, lower.tail = TRUE, log.p = FALSE)
round(pbinom(2,size=4,prob=0.5,lower.tail=FALSE),2)
pbinom (3,size=4,prob = 0.5)
n <- 9
mn <- 1100
s <- 30
a <- 0.05
mn + c(1, -1) * qt(1 - (a/2), n-1) * s/sqrt(n)
p1 <- 1/100
p2 <- 10/1787
n <- 1787
s <- sqrt((p1 * (1 - p1) / n))
test_z = (p1 - p2) / s
pnorm(test_z, lower.tail = FALSE)
mean.diff = -3-1
df = (9 + 9 - 2)
m_tr = -3
m_pb = 1
s_tr = 1.5
s_pb = 1.8
pooled.var = (s_tr^2 * 9 + s_pb^2 * 9)/df
se.diff = sqrt(pooled.var/9 + pooled.var/9)
t.obt = mean.diff / se.diff
t.obt
n <- 100
mn <- .01
s <- .04
power.t.test(n = 100, delta = .01, sd = .04, sig.level = 0.05, type = "one.sample", alt = "one.sided")
baseline <- c(140, 138, 150, 148, 135)
week2 <- c(132, 135, 151, 146, 130)
results <- data.frame(baseline, week2)
t.test(x = results$baseline, y = results$week2, alternative = "two.sided", paired = TRUE)
n <- 9
mn <- 1100
s <- 30
a <- 0.05
mn + c(1, -1) * qt(1 - (a/2), n-1) * s/sqrt(n)
binom.test(c(3, 1), p = 0.5, alternative = "greater")
# get probabilites per day
p1 <- 1/100
p2 <- 10/1787
n <- 1787
#s <- sqrt((p1 * (1 - p1)/n))
#ppois(p2, 1, lower.tail = TRUE)
# When the population size is much larger (at least 10 times larger) than the sample size, the standard deviation can be approximated by:
# ??p = sqrt[ P * ( 1 - P ) / n ] where n is the sample size
s <- sqrt((p1 * (1 - p1) / n))
# get the releveant quantile(lower) expressed in standard deviations
test_z = (p1 - p2) / s
pnorm(test_z, lower.tail = FALSE)
?power.t.test
delta <- .01
s <- .04
power.t.test(delta = .01, sd = .04, sig.level = 0.05, type = "one.sample", alt = "one.sided")
mn <- .01
s <- .04
p <- .9
power.t.test(delta = .01, sd = .04, power = .9, type = "one.sample", alternative = "one.sided")
mean.diff = -3-1
df = (9 + 9 - 2)
m_tr = -3
m_pb = 1
s_tr = 1.5
s_pb = 1.8
pooled.var = (s_tr^2 * 9 + s_pb^2 * 9)/df
se.diff = sqrt(pooled.var/9 + pooled.var/9)
t.obt = mean.diff / se.diff
t.obt
n1 <- n2 <- 9
x1 <- -3  ##treated
x2 <- 1  ##placebo
s1 <- 1.5  ##treated
s2 <- 1.8  ##placebo
spsq <- ( (n1 - 1) * s1^2 + (n2 - 1) * s2^2) / (n1 + n2 - 2)
t=(x1-x2)/(spsq*sqrt(1/n1 + 1/n2))
2*pt(t,n1+n2-2)
baseline <- c(140, 138, 150, 148, 135)
week2 <- c(132, 135, 151, 146, 130)
results <- data.frame(baseline, week2)
s <- sqrt((p1 * (1 - p1) / n))
s
swirl()
library(swirl)
swirl()
install_from_swirl("Regression Models")
swirl()
plot(child~parent,galton)
plot(jitter(child,4) ~ parent,galton)
regrline<-lm(child~parent,galton)
abline(regrline, lwd=3, col='red')
summary(regrline)
quit()
library(swirl)
swirl()
regrline<-lm(child~parent,galton)
fit<-lm(child~parent,galton)
fit$residuals
summary(fit)
mean(fit$residuals)
cov(fit$residuals,galton$parent)
ols.ic<- fit$coef[1]
ols.slope<- fit$coef[2]
chs - lhs
vhs-lhs
rhs-lhs
lhs-rhs
all.equal(lhs,rhs)
varChild<-var(Ols)
varChild<-var(lhs)
varchild<-var(galton$child)
varChild<-var(galton$child)
varRes<-Var(fit$residuals)
varRes<-var(fit$residuals)
varEst<-var(est)
varEst<-var(ols.slope)
varEst <- var(est(ols.slope, ols.ic))
varChild<-all.equal(varRes,varEst)
"all.equal(varChild,varEst+varRes)
varChild<- all.equal(varChild,varEst+varRes)
all.equal(varChild,varEst+varRes)
efit <- lm(accel ~ mag+dist, attenu)
mean(residuls)
mean(efit)
mean(efit$residuals)
cov(attenu$mag)
cov(efit$residuals,attenu$mag)
cov(efit$residuals,attenu$dist)
cor(gpa_nor,gch_nor)
l_nor<-lm(gpa_nor~gch_nor)
l_nor<-lm(gch_nor~gpa_nor)
fit<-lm(child~parent,galton)
sum(fit$residuals)/926
n
sqrt(sum(fit$residuals^2) / (n - 2))
summary(fit)$sigma
sqrt(deviance(fit)/(n-2))
mu<-mean(galton$child)
sTot<-
sTot<-mu
sTot <- sum((galton$child-mu)^2)
sRes<-deviance(sum(galton$residuals)^2)
sRes<-deviance(sum(galton$residual)^2)
sRes<-deviance(sum(residuals)^2)
sRes<-sqrt(deviance(residuals)/(n-2))
sRes
sRes<-deviance(residuals)
skip()
1- sRes/sTot
summary(fit)$r
summary(fit)$r.squared
cor(galton$child,galton$parent)
cor(galton$parent,galton$child)^2
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
plot(x,w)
z<- lm(x~w)
z
library(datasets)
data(mtcars)
?lm
lm(mtcars$mpg~mtcars$weight)
head(mtcars)
lm(mtcars$mpg~mtcars$t)
lm(mtcars$mpg~mtcars$wt)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm(y~x)
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
plot(x,w)
cor(x,w)
fit <-lm(x,w)
fit <-lm(x~w)
abline(fit)
plot(x,w)
abline(fit)
cor(.6,1.5)
?cor
cor(0.6,1.5)
x <- c(8.58, 10.46, 9.01, 9.64, 8.86)
hist(x)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm(y~x)
lm(x~y)
getwd()
getwd("/Users/dame81/Desktop/DS_classes/Coursework/RepData_PeerAssessment1")
setwd("/Users/dame81/Desktop/DS_classes/Coursework/RepData_PeerAssessment1")
activity <- read.csv("activity.csv")
summary(activity)
new_data= data.frame(activity)
for (i in 1:dim(new_data)[1]) {
if (is.na(new_data[i, ]$steps)) {
interval = new_data[i, ]$interval
avg_value = interval_max[interval_max$Interval == interval, ]$Mean
new_data[i, ]$steps = avg_value
}
}
ag2_activity<-aggregate(steps ~ interval, activity, mean)
interval_max<- which.max(ag2_activity$steps)
interval_max
new_data= data.frame(activity)
for (i in 1:dim(new_data)[1]) {
if (is.na(new_data[i, ]$steps)) {
interval = new_data[i, ]$interval
avg_value = interval_max[interval_max$Interval == interval, ]$Mean
new_data[i, ]$steps = avg_value
}
}
ag2_activity<-aggregate(steps ~ interval, activity, mean)
ag2_activity
mean_interval<- as.vector(ag2_activity)
mean_interval
temp_interval<-rep(mean_interval,61)
mean_interal[!is.na(ag2_activity$steps)] = 1
mean_interval[!is.na(ag2_activity$steps)] = 1
mean_interval[!is.na(activity$steps)] = 1
temp_interval
temp_interval[!is.na(activity$steps)] = 1
tmp_steps<-as.vector(activity$steps)
temp_steps[is.na(tmp_steps)] = 1
temp_steps[is.na(temp_steps)] = 1
tmp_steps[is.na(tmp_steps)] = 1
good_data<- activity
good_data$steps<-temp_interval*tmp_steps
mean(activity$steps, na.rm=T)
impute_activity <- activity
impute_activity$steps[is.na(impute_activity$steps)] <- mean(impute_activity$steps, na.rm = T)
colSums(is.na(impute_activity))
impute_steps<- tapply(impute_activity$steps, impute_activity$date, sum)
impute_melt<- melt(impute_steps)
library(reshape2)
impute_melt<- melt(impute_steps)
impute_melt
head(impute_melt)
hist(impute_melt$Steps, main = "Histogram of Total Number of Steps per Day for Impute Value”,
xlab = " Steps per Day", ylab = "Frequency", col = “red",
breaks = 35)
hist(impute_melt$Steps, main = "Histogram of Total Number of Steps per Day for Impute Value”,xlab = " Steps per Day", ylab = "Frequency", col = “red", breaks = 35)
hist(impute_melt$Steps, main = "Histogram of Total Number of Steps per Day for Impute Value”,xlab = " Steps per Day", ylab = "Frequency", col = “red", breaks = 35)
hist(impute_melt$Steps, main = "Histogram of Total Number of Steps per Day for Impute Value”,          xlab = " Steps per Day", ylab = "Frequency", col = “red", breaks = 35)
hist(impute_melt$Steps, main = "Histogram of Total Number of Steps per Day for Impute Data",
xlab = "Total Number of Steps per Day", ylab = "Frequency", col = “red",
breaks = 30)
hist(impute_melt$Steps, main = "Histogram of Total Number of Steps per Day for Impute Data",
xlab = "Total Number of Steps per Day", ylab = "Frequency", col = “red",
breaks = 30)
impute_melt
hist(impute_melt, main = "Histogram of Total Number of Steps per Day for Impute Value”,xlab = " Steps per Day", ylab = "Frequency", col = “red", breaks = 35)
hist(impute_melt, breaks=35, main = "Histogram of Total Number of Steps per Day for Impute Value”,xlab = " Steps per Day", ylab = "Frequency", col = “red")
hist(impute_melt, breaks=35, main = "Total Number of Steps per Day for Impute Value”,xlab = " Steps per Day", ylab = "Frequency", col = “red")
hist(impute_melt,breaks=35,col="red")
hist(impute_melt$Steps,breaks=35,col="red")
head(impute_melt)
names(impute_melt) <- c("Date", "Steps")
head(impute_melt)
hist(impute_melt, breaks=35, main = "Total Number of Steps per Day for Impute Value”,xlab = " Steps per Day", ylab = "Frequency", col = “red")
hist(impute_melt$Steps, breaks=35, main = "Total Number of Steps per Day for Impute Value”,xlab = " Steps per Day", ylab = "Frequency", col = “red")
hist(impute_melt$Steps,breaks=35,col="red")
hist(impute_melt$Steps,breaks=30,col="red", main="Number of Steps per Dy for Impute Data",xlab="Steps per Day")
